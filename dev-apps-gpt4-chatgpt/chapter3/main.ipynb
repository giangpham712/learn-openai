{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 3. Building Apps with GPT-4 and ChatGPT"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6eb304516c0a8b21"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import settings\n",
    "import openai"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-05T15:13:27.219954600Z",
     "start_time": "2024-01-05T15:13:26.746454Z"
    }
   },
   "id": "ddac61c25bdc01be"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Building a News Generator Solution"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d2e338ef8b08ef2"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey there, fellow nature enthusiasts! Let's take a moment to appreciate the wonders of our world. Have you ever wondered why the sky is so beautifully blue? Well, it's all thanks to a little something called Rayleigh scattering. When sunlight reaches Earth's atmosphere, its short blue waves get scattered in all directions, making the sky appear blue to our eyes. And what about that lush green grass beneath our feet? The green color comes from chlorophyll, a pigment in plants that helps them capture sunlight for photosynthesis. So next time you're out and about, take a moment to marvel at the fact that our sky is blue and our grass is green. Nature truly is amazing!\n"
     ]
    }
   ],
   "source": [
    "from examples import news_generator\n",
    "print(\n",
    "    news_generator.assist_journalist(\n",
    "        [\"The sky is blue\", \"The grass is green\"], \"informal\", 100, \"blogpost\"\n",
    "    )\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-05T03:12:47.125211Z",
     "start_time": "2024-01-05T03:12:39.477912900Z"
    }
   },
   "id": "d912bf9c428dc4cc"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exciting news! O'Reilly has just released an incredible book titled \"Developing Apps with GPT-4 and ChatGPT.\" This latest publication delves into the world of ChatGPT, offering readers insights into developing amazing applications. Make sure to grab a copy and explore the limitless potential of this groundbreaking technology!\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    news_generator.assist_journalist(\n",
    "        facts=[\n",
    "            \"A book on ChatGPT has been published last week\",\n",
    "            \"The title is Developing Apps with GPT-4 and ChatGPT\",\n",
    "            \"The publisher is O'Reilly.\",\n",
    "        ],\n",
    "        tone=\"excited\",\n",
    "        length_words=50,\n",
    "        style=\"news flash\",\n",
    "    )\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T06:33:16.796419200Z",
     "start_time": "2024-01-04T06:33:13.208827100Z"
    }
   },
   "id": "fe857cf37cc84498"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Summarizing YouTube Videos"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c7550fc2de26d768"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text discusses the new Toyota Land Cruiser 250, highlighting its retro restyling and rugged design. The vehicle is described as having a sense of purpose, with boxy surfacing, straight edges, and solid-looking features. It is mentioned that the Land Cruiser 250 is smaller than its predecessor and offers various engine options, including a diesel version for the UK market. The interior is described as solid and masculine, with large physical buttons and a big touchscreen. The car also offers off-road capabilities, including terrain modes and remote disconnect of anti-roll bars. The text concludes by mentioning that the Land Cruiser 250 will be available in the UK by 2024, with an estimated starting price of £45,000. Overall, the writer expresses excitement about the Land Cruiser 250, noting that it brings the Land Cruiser badge into the modern era.\n"
     ]
    }
   ],
   "source": [
    "from examples import youtube_summarizer\n",
    "with open(\"transcript.txt\", \"r\") as f:\n",
    "    transcript = f.read()\n",
    "\n",
    "    \n",
    "    \n",
    "youtube_summarizer.summarize_transcript(transcript)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-05T12:28:31.672789600Z",
     "start_time": "2024-01-05T12:28:23.381289600Z"
    }
   },
   "id": "7a0d05929051ab95"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Creating an Expert for Zelda BOTW"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ab5432d3466c3f06"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The idea is to use ChatGPT or GPT-4 models for information restitution, but not information retrieval: we do not expect the AI model to know the answer to the question. Rather, we ask it to formulate a well-thought answer based on text extracts we think could match the question.\n",
    "\n",
    "![alt text](dagc_0305-1.png \"The principle of a ChatGPT-like solution powered with your own data\")\n",
    "\n",
    "Three components:\n",
    "- An intent service \n",
    "    - detect the intent of the question\n",
    "    - detect whether the question from the user does not respect OpenAI’s policy, or perhaps contains sensitive information\n",
    "- An information retrieival service\n",
    "    - take the output from the intent service and retrieve the correct information\n",
    "- A response service\n",
    "    - take the output of the information retrieval service and generate from it an answer to the user’s question"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a04a052b3ef3bc3f"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "ename": "ResponseError",
     "evalue": "unknown command `FT.CREATE`, with args beginning with: `embeddings-index`, `ON`, `HASH`, `PREFIX`, `1`, `doc`, `SCORE`, `1.0`, `SCHEMA`, `text`, `TEXT`, `WEIGHT`, `1.0`, `vector`, `VEC`, ",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mResponseError\u001B[0m                             Traceback (most recent call last)",
      "File \u001B[1;32mG:\\Giang\\Learn\\learn-openai\\dev-apps-gpt4-chatgpt\\chapter3\\examples\\question_answering\\data_service.py:42\u001B[0m, in \u001B[0;36mDataService.load_data_to_redis\u001B[1;34m(self, embeddings)\u001B[0m\n\u001B[0;32m     41\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 42\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mredis_client\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mft\u001B[49m\u001B[43m(\u001B[49m\u001B[43mINDEX_NAME\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minfo\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     43\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIndex already exists\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32mG:\\Giang\\Learn\\learn-openai\\.venv\\Lib\\site-packages\\redis\\commands\\search\\commands.py:450\u001B[0m, in \u001B[0;36mSearchCommands.info\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    443\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    444\u001B[0m \u001B[38;5;124;03mGet info an stats about the the current index, including the number of\u001B[39;00m\n\u001B[0;32m    445\u001B[0m \u001B[38;5;124;03mdocuments, memory consumption, etc\u001B[39;00m\n\u001B[0;32m    446\u001B[0m \n\u001B[0;32m    447\u001B[0m \u001B[38;5;124;03mFor more information see `FT.INFO <https://redis.io/commands/ft.info>`_.\u001B[39;00m\n\u001B[0;32m    448\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m--> 450\u001B[0m res \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexecute_command\u001B[49m\u001B[43m(\u001B[49m\u001B[43mINFO_CMD\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mindex_name\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    451\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_parse_results(INFO_CMD, res)\n",
      "File \u001B[1;32mG:\\Giang\\Learn\\learn-openai\\.venv\\Lib\\site-packages\\redis\\client.py:536\u001B[0m, in \u001B[0;36mRedis.execute_command\u001B[1;34m(self, *args, **options)\u001B[0m\n\u001B[0;32m    535\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 536\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mconn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mretry\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcall_with_retry\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    537\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_send_command_parse_response\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    538\u001B[0m \u001B[43m            \u001B[49m\u001B[43mconn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcommand_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43moptions\u001B[49m\n\u001B[0;32m    539\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    540\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43merror\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_disconnect_raise\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43merror\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    541\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    542\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
      "File \u001B[1;32mG:\\Giang\\Learn\\learn-openai\\.venv\\Lib\\site-packages\\redis\\retry.py:46\u001B[0m, in \u001B[0;36mRetry.call_with_retry\u001B[1;34m(self, do, fail)\u001B[0m\n\u001B[0;32m     45\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 46\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mdo\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     47\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_supported_errors \u001B[38;5;28;01mas\u001B[39;00m error:\n",
      "File \u001B[1;32mG:\\Giang\\Learn\\learn-openai\\.venv\\Lib\\site-packages\\redis\\client.py:537\u001B[0m, in \u001B[0;36mRedis.execute_command.<locals>.<lambda>\u001B[1;34m()\u001B[0m\n\u001B[0;32m    535\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    536\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m conn\u001B[38;5;241m.\u001B[39mretry\u001B[38;5;241m.\u001B[39mcall_with_retry(\n\u001B[1;32m--> 537\u001B[0m         \u001B[38;5;28;01mlambda\u001B[39;00m: \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_send_command_parse_response\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    538\u001B[0m \u001B[43m            \u001B[49m\u001B[43mconn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcommand_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43moptions\u001B[49m\n\u001B[0;32m    539\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m,\n\u001B[0;32m    540\u001B[0m         \u001B[38;5;28;01mlambda\u001B[39;00m error: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_disconnect_raise(conn, error),\n\u001B[0;32m    541\u001B[0m     )\n\u001B[0;32m    542\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
      "File \u001B[1;32mG:\\Giang\\Learn\\learn-openai\\.venv\\Lib\\site-packages\\redis\\client.py:513\u001B[0m, in \u001B[0;36mRedis._send_command_parse_response\u001B[1;34m(self, conn, command_name, *args, **options)\u001B[0m\n\u001B[0;32m    512\u001B[0m conn\u001B[38;5;241m.\u001B[39msend_command(\u001B[38;5;241m*\u001B[39margs)\n\u001B[1;32m--> 513\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparse_response\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcommand_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mG:\\Giang\\Learn\\learn-openai\\.venv\\Lib\\site-packages\\redis\\client.py:553\u001B[0m, in \u001B[0;36mRedis.parse_response\u001B[1;34m(self, connection, command_name, **options)\u001B[0m\n\u001B[0;32m    552\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 553\u001B[0m         response \u001B[38;5;241m=\u001B[39m \u001B[43mconnection\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_response\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    554\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m ResponseError:\n",
      "File \u001B[1;32mG:\\Giang\\Learn\\learn-openai\\.venv\\Lib\\site-packages\\redis\\connection.py:524\u001B[0m, in \u001B[0;36mAbstractConnection.read_response\u001B[1;34m(self, disable_decoding, disconnect_on_error, push_request)\u001B[0m\n\u001B[0;32m    523\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 524\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m response\n\u001B[0;32m    525\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
      "\u001B[1;31mResponseError\u001B[0m: unknown command `FT.INFO`, with args beginning with: `embeddings-index`, ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mResponseError\u001B[0m                             Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[7], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mexamples\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mquestion_answering\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m main \u001B[38;5;28;01mas\u001B[39;00m question_answering\n\u001B[1;32m----> 3\u001B[0m \u001B[43mquestion_answering\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mWhat are the Bokoblins?\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mExplorersGuide.pdf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mG:\\Giang\\Learn\\learn-openai\\dev-apps-gpt4-chatgpt\\chapter3\\examples\\question_answering\\main.py:8\u001B[0m, in \u001B[0;36mrun\u001B[1;34m(question, file)\u001B[0m\n\u001B[0;32m      6\u001B[0m data_service \u001B[38;5;241m=\u001B[39m DataService()\n\u001B[0;32m      7\u001B[0m data \u001B[38;5;241m=\u001B[39m data_service\u001B[38;5;241m.\u001B[39mpdf_to_embeddings(file)\n\u001B[1;32m----> 8\u001B[0m \u001B[43mdata_service\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload_data_to_redis\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     10\u001B[0m intent_service \u001B[38;5;241m=\u001B[39m IntentService()\n\u001B[0;32m     11\u001B[0m intents \u001B[38;5;241m=\u001B[39m intent_service\u001B[38;5;241m.\u001B[39mget_intent(question)\n",
      "File \u001B[1;32mG:\\Giang\\Learn\\learn-openai\\dev-apps-gpt4-chatgpt\\chapter3\\examples\\question_answering\\data_service.py:58\u001B[0m, in \u001B[0;36mDataService.load_data_to_redis\u001B[1;34m(self, embeddings)\u001B[0m\n\u001B[0;32m     50\u001B[0m     text_embedding \u001B[38;5;241m=\u001B[39m VectorField(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvector\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFLAT\u001B[39m\u001B[38;5;124m\"\u001B[39m, {\n\u001B[0;32m     51\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTYPE\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFLOAT32\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m     52\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDIM\u001B[39m\u001B[38;5;124m\"\u001B[39m: vector_dim,\n\u001B[0;32m     53\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDISTANCE_METRIC\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCOSINE\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m     54\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mINITIAL_CAP\u001B[39m\u001B[38;5;124m\"\u001B[39m: vector_number\n\u001B[0;32m     55\u001B[0m     })\n\u001B[0;32m     56\u001B[0m     fields \u001B[38;5;241m=\u001B[39m [text, text_embedding]\n\u001B[1;32m---> 58\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mredis_client\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mft\u001B[49m\u001B[43m(\u001B[49m\u001B[43mINDEX_NAME\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreate_index\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     59\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfields\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfields\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     60\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdefinition\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mIndexDefinition\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     61\u001B[0m \u001B[43m            \u001B[49m\u001B[43mprefix\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43mPREFIX\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindex_type\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mIndexType\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mHASH\u001B[49m\n\u001B[0;32m     62\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     63\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     65\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m embedding \u001B[38;5;129;01min\u001B[39;00m embeddings:\n\u001B[0;32m     66\u001B[0m     key \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mPREFIX\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m:\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mstr\u001B[39m(embedding[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mid\u001B[39m\u001B[38;5;124m'\u001B[39m])\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n",
      "File \u001B[1;32mG:\\Giang\\Learn\\learn-openai\\.venv\\Lib\\site-packages\\redis\\commands\\search\\commands.py:221\u001B[0m, in \u001B[0;36mSearchCommands.create_index\u001B[1;34m(self, fields, no_term_offsets, no_field_flags, stopwords, definition, max_text_fields, temporary, no_highlight, no_term_frequencies, skip_initial_scan)\u001B[0m\n\u001B[0;32m    218\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[0;32m    219\u001B[0m     args \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m fields\u001B[38;5;241m.\u001B[39mredis_args()\n\u001B[1;32m--> 221\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexecute_command\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mG:\\Giang\\Learn\\learn-openai\\.venv\\Lib\\site-packages\\redis\\client.py:536\u001B[0m, in \u001B[0;36mRedis.execute_command\u001B[1;34m(self, *args, **options)\u001B[0m\n\u001B[0;32m    533\u001B[0m conn \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconnection \u001B[38;5;129;01mor\u001B[39;00m pool\u001B[38;5;241m.\u001B[39mget_connection(command_name, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moptions)\n\u001B[0;32m    535\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 536\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mconn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mretry\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcall_with_retry\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    537\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_send_command_parse_response\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    538\u001B[0m \u001B[43m            \u001B[49m\u001B[43mconn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcommand_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43moptions\u001B[49m\n\u001B[0;32m    539\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    540\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43merror\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_disconnect_raise\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43merror\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    541\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    542\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    543\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconnection:\n",
      "File \u001B[1;32mG:\\Giang\\Learn\\learn-openai\\.venv\\Lib\\site-packages\\redis\\retry.py:46\u001B[0m, in \u001B[0;36mRetry.call_with_retry\u001B[1;34m(self, do, fail)\u001B[0m\n\u001B[0;32m     44\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m     45\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 46\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mdo\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     47\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_supported_errors \u001B[38;5;28;01mas\u001B[39;00m error:\n\u001B[0;32m     48\u001B[0m         failures \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "File \u001B[1;32mG:\\Giang\\Learn\\learn-openai\\.venv\\Lib\\site-packages\\redis\\client.py:537\u001B[0m, in \u001B[0;36mRedis.execute_command.<locals>.<lambda>\u001B[1;34m()\u001B[0m\n\u001B[0;32m    533\u001B[0m conn \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconnection \u001B[38;5;129;01mor\u001B[39;00m pool\u001B[38;5;241m.\u001B[39mget_connection(command_name, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moptions)\n\u001B[0;32m    535\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    536\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m conn\u001B[38;5;241m.\u001B[39mretry\u001B[38;5;241m.\u001B[39mcall_with_retry(\n\u001B[1;32m--> 537\u001B[0m         \u001B[38;5;28;01mlambda\u001B[39;00m: \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_send_command_parse_response\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    538\u001B[0m \u001B[43m            \u001B[49m\u001B[43mconn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcommand_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43moptions\u001B[49m\n\u001B[0;32m    539\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m,\n\u001B[0;32m    540\u001B[0m         \u001B[38;5;28;01mlambda\u001B[39;00m error: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_disconnect_raise(conn, error),\n\u001B[0;32m    541\u001B[0m     )\n\u001B[0;32m    542\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    543\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconnection:\n",
      "File \u001B[1;32mG:\\Giang\\Learn\\learn-openai\\.venv\\Lib\\site-packages\\redis\\client.py:513\u001B[0m, in \u001B[0;36mRedis._send_command_parse_response\u001B[1;34m(self, conn, command_name, *args, **options)\u001B[0m\n\u001B[0;32m    509\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    510\u001B[0m \u001B[38;5;124;03mSend a command and parse the response\u001B[39;00m\n\u001B[0;32m    511\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    512\u001B[0m conn\u001B[38;5;241m.\u001B[39msend_command(\u001B[38;5;241m*\u001B[39margs)\n\u001B[1;32m--> 513\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparse_response\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcommand_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mG:\\Giang\\Learn\\learn-openai\\.venv\\Lib\\site-packages\\redis\\client.py:553\u001B[0m, in \u001B[0;36mRedis.parse_response\u001B[1;34m(self, connection, command_name, **options)\u001B[0m\n\u001B[0;32m    551\u001B[0m         options\u001B[38;5;241m.\u001B[39mpop(NEVER_DECODE)\n\u001B[0;32m    552\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 553\u001B[0m         response \u001B[38;5;241m=\u001B[39m \u001B[43mconnection\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_response\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    554\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m ResponseError:\n\u001B[0;32m    555\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m EMPTY_RESPONSE \u001B[38;5;129;01min\u001B[39;00m options:\n",
      "File \u001B[1;32mG:\\Giang\\Learn\\learn-openai\\.venv\\Lib\\site-packages\\redis\\connection.py:524\u001B[0m, in \u001B[0;36mAbstractConnection.read_response\u001B[1;34m(self, disable_decoding, disconnect_on_error, push_request)\u001B[0m\n\u001B[0;32m    522\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response, ResponseError):\n\u001B[0;32m    523\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 524\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m response\n\u001B[0;32m    525\u001B[0m     \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    526\u001B[0m         \u001B[38;5;28;01mdel\u001B[39;00m response  \u001B[38;5;66;03m# avoid creating ref cycles\u001B[39;00m\n",
      "\u001B[1;31mResponseError\u001B[0m: unknown command `FT.CREATE`, with args beginning with: `embeddings-index`, `ON`, `HASH`, `PREFIX`, `1`, `doc`, `SCORE`, `1.0`, `SCHEMA`, `text`, `TEXT`, `WEIGHT`, `1.0`, `vector`, `VEC`, "
     ]
    }
   ],
   "source": [
    "from examples.question_answering import main as question_answering\n",
    "\n",
    "question_answering.run(\"What are the Bokoblins?\", \"ExplorersGuide.pdf\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-05T12:28:51.732826500Z",
     "start_time": "2024-01-05T12:28:40.158827700Z"
    }
   },
   "id": "2fe8dde75a7f9d4c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Voice control"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4e809f499a833ff9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Assistant with GPT-3.5 Turbo\n",
    "\n",
    "![alt text](dagc_0306.png \"The OpenAI API is used to detect the intent of the user’s input \")\n",
    "\n",
    "Four states to answer questions:\n",
    "\n",
    "\n",
    "- QUESTION - We have detected that the user has asked a question.\n",
    "- ANSWER - We are ready to answer the question.\n",
    "- MORE - We need more information.\n",
    "- OTHER - We do not want to continue the discussion (we cannot answer the question).\n",
    "\n",
    "\n",
    "![alt text](dagc_0307.png \"An example diagram of a state machine\")\n",
    "\n",
    "![alt text](dagc_0308.png \"A state machine diagram for answering questions and emailing\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "47e977e46950a164"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from examples.voice_control import main as voice_control"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-05T15:13:35.229454100Z",
     "start_time": "2024-01-05T15:13:30.365454Z"
    }
   },
   "id": "e069152b60b08e03",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": ""
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interface = gr.Interface(\n",
    "    fn=voice_control.run,\n",
    "    live=True,\n",
    "    inputs=gr.Audio(sources=[\"microphone\"], type=\"filepath\"),\n",
    "    outputs=\"text\",\n",
    ")\n",
    "\n",
    "interface.launch()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-05T13:46:24.374007800Z",
     "start_time": "2024-01-05T13:46:19.144010800Z"
    }
   },
   "id": "47d2046f15686a39",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7861\n"
     ]
    }
   ],
   "source": [
    "interface.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-05T12:41:46.230540800Z",
     "start_time": "2024-01-05T12:41:46.223539300Z"
    }
   },
   "id": "1330cc1b3041b50b",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'user', 'content': 'Classify the intent of one next input.              Is it: WRITE_EMAIL, QUESTION, OTHER ? Only answer one word.'}, {'role': 'user', 'content': 'Write an email to John'}]\n",
      "WRITE_EMAIL\n",
      "[{'role': 'user', 'content': 'Classify the intent of one next input.              Is it: WRITE_EMAIL, QUESTION, OTHER ? Only answer one word.'}, {'role': 'user', 'content': 'Write an email to John'}, {'role': 'assistant', 'content': 'WRITE_EMAIL'}, {'role': 'user', 'content': 'If the subject or recipient or message is missing,                    answer \"MORE\". Else if you have all the information,                    answer \"ACTION_WRITE_EMAIL |                   subject:subject, recipient:recipient, message:message\".'}]\n",
      "MORE\n",
      "[{'role': 'user', 'content': 'Classify the intent of one next input.              Is it: WRITE_EMAIL, QUESTION, OTHER ? Only answer one word.'}, {'role': 'user', 'content': 'Write an email to John'}, {'role': 'assistant', 'content': 'WRITE_EMAIL'}, {'role': 'user', 'content': 'If the subject or recipient or message is missing,                    answer \"MORE\". Else if you have all the information,                    answer \"ACTION_WRITE_EMAIL |                   subject:subject, recipient:recipient, message:message\".'}, {'role': 'assistant', 'content': 'MORE'}, {'role': 'user', 'content': 'Now ask for more information'}]\n",
      "What is the subject of the email?\n"
     ]
    },
    {
     "data": {
      "text/plain": "'What is the subject of the email?'"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voice_control.start(\"Write an email to John\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-05T15:13:38.934454200Z",
     "start_time": "2024-01-05T15:13:36.748454200Z"
    }
   },
   "id": "e9854d70fecbc400",
   "execution_count": 3
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
